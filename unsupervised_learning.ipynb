{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the path for data is data/json/\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd # pandas pour avoir un format (DataFrame) confortable pour les données.\n",
    "import numpy as np # Numpy pour le calcul du taux de bonnes prédictions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # outil pour traiter le texte\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "path_to_data = os.path.join(\"data/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Définition de la pipeline\n",
    "km_clf = Pipeline([('vect', TfidfVectorizer(analyzer = \"word\",stop_words='english',max_features = 3000)),\n",
    "                   ('clf', KMeans(n_clusters=5, init='k-means++', max_iter=20, n_init=5, n_jobs=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apprentissage avec les k-means\n",
    "file_list =[f for f in os.listdir(\"data/json/\")]\n",
    "\n",
    "tps1 = time.clock() \n",
    "\n",
    "f_out = \"data/clustering_results/result.json\"\n",
    "with open(f_out,'w') as out:  \n",
    "    for f in file_list:\n",
    "        with open(path_to_data + f) as data:\n",
    "            dt = json.load(data)\n",
    "    \n",
    "        review_id = [review[\"ReviewID\"] for review in dt[\"Reviews\"]]\n",
    "        review_to_anayse = [review[\"Content\"] for review in dt[\"Reviews\"]]\n",
    "\n",
    "        if len(review_to_anayse)>5:\n",
    "            km_clf.fit(review_to_anayse)\n",
    "            output = {\"HotelID\":dt[\"HotelInfo\"][\"HotelID\"], \"Review_Clustering\":dict(zip(review_id,km_clf.named_steps['clf'].labels_))}\n",
    "            json.dump(output,out)\n",
    "        else:\n",
    "            print \"File\",f,\"is empty\"\n",
    "            tmp = ['null' for i in range(len(review_to_anayse))]\n",
    "            output = {\"HotelID\":dt[\"HotelInfo\"][\"HotelID\"], \"Review_Clustering\":dict(zip(review_id,tmp))}\n",
    "            json.dump(output,out)\n",
    "            \n",
    "tps2 = time.clock()\n",
    "print\"\"\n",
    "print \"Done in \",tps2 - tps1,\" seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clustering avec la Non-Negative Matrix Factorization\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 50\n",
    "n_topics = 10\n",
    "n_top_words = 5                                                                                                                                                             \n",
    "\n",
    "file_list =[f for f in os.listdir(\"data/json/\")]\n",
    "samples_id = [file_list[id] for id in random.sample(range(len(file_list)), n_samples)]\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "reviews = []\n",
    "for f in samples_id:\n",
    "    with open(path_to_data + f) as data:\n",
    "        dt = json.load(data)\n",
    "\n",
    "    reviews.append([review[\"Content\"] for review in dt[\"Reviews\"]])\n",
    "\n",
    "reviews = [el for index in range(len(reviews))\n",
    "              for el in reviews[index]]\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer = \"word\",stop_words=english_stop_words,max_features = n_features)\n",
    "\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(reviews)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\"n_samples=%d and n_features=%d...\"% (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, tol = 1e-5,random_state=1).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from urllib2 import urlopen \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# Plot it out\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(nmf.components_, cmap=plt.cm.Blues, alpha=0.8)\n",
    "\n",
    "##################################################\n",
    "## FORMAT ##\n",
    "##################################################\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,17)\n",
    "\n",
    "# turn off the frame\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_yticks(np.arange(nmf.components_.shape[0])+0.5, minor=False)\n",
    "ax.set_xticks(np.arange(nmf.components_.shape[1])+0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "# Set the labels\n",
    "\n",
    "# label source:https://en.wikipedia.org/wiki/Basketball_statistics\n",
    "labels_x = [word for word in tfidf_feature_names]\n",
    "labels_y = ['Topic '+ str(i) for i in range(nmf.components_.shape[0])]\n",
    "# note I could have used nba_sort.columns but made \"labels\" instead\n",
    "ax.set_xticklabels(labels_x, minor=False) \n",
    "ax.set_yticklabels(labels_y, minor=False)\n",
    "\n",
    "# rotate the \n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "# Turn off all the ticks\n",
    "ax = plt.gca()\n",
    "\n",
    "for t in ax.xaxis.get_major_ticks(): \n",
    "    t.tick1On = False \n",
    "    t.tick2On = False \n",
    "for t in ax.yaxis.get_major_ticks(): \n",
    "    t.tick1On = False \n",
    "    t.tick2On = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Définition de la pipeline\n",
    "km_clf = Pipeline([('vect', TfidfVectorizer(analyzer = \"word\",stop_words='english',max_features = n_features)),\n",
    "                   ('clf', KMeans(init=nmf.components_, max_iter=20, n_init=5))])\n",
    "\n",
    "# Apprentissage avec les k-means\n",
    "file_list =[f for f in os.listdir(\"data/json/\")]\n",
    "\n",
    "tps1 = time.clock() \n",
    "\n",
    "# Liste des fichiers à éviter\n",
    "to_avoid = ['1153745.json','231512.json','258610.json']\n",
    "\n",
    "count =0\n",
    "for f in file_list:\n",
    "    count = count + 1\n",
    "    if count==5000:\n",
    "        break\n",
    "            \n",
    "    with open(path_to_data + f) as data:\n",
    "        dt = json.load(data)\n",
    "    \n",
    "    review_id = [review[\"ReviewID\"] for review in dt[\"Reviews\"]]\n",
    "    review_to_anayse = [review[\"Content\"] for review in dt[\"Reviews\"]]\n",
    "        \n",
    "    f_out = \"data/clustering_results/\"+dt[\"HotelInfo\"][\"HotelID\"]+\".json\"\n",
    "        \n",
    "    if len(review_to_anayse)>nmf.components_.shape[0] and not f in to_avoid:\n",
    "        km_clf.fit(review_to_anayse)\n",
    "        output = pd.DataFrame( data={\"id\":review_id, \"cluster\":km_clf.named_steps['clf'].labels_} ).to_csv( f_out, index=False, quoting=3 )\n",
    "    else:\n",
    "        tmp = ['null' for i in range(len(review_to_anayse))]\n",
    "        output = pd.DataFrame( data={\"id\":review_id, \"cluster\":tmp} ).to_csv( f_out, index=False, quoting=3 )\n",
    "            \n",
    "tps2 = time.clock()\n",
    "print\"\"\n",
    "print \"Done in \",tps2 - tps1,\" seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list =[f for f in os.listdir(\"data/clustering_results/\")]\n",
    "\n",
    "heat = []\n",
    "\n",
    "count = 0\n",
    "for f in file_list:\n",
    "    count = count + 1\n",
    "    \n",
    "    if count == 31:\n",
    "        break\n",
    "        \n",
    "    data = pd.read_csv(\"data/clustering_results/\"+f, header=0, delimiter=\",\")\n",
    "    \n",
    "    clusters = [0 for i in range(n_topics)]\n",
    "\n",
    "    for i in data[\"cluster\"]:\n",
    "        if i!='null':\n",
    "            clusters[i] += 1\n",
    "    \n",
    "    heat.append(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heat = np.array(heat)\n",
    "# Plot it out\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(heat, cmap=plt.cm.Blues, alpha=0.8)\n",
    "\n",
    "##################################################\n",
    "## FORMAT ##\n",
    "##################################################\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,17)\n",
    "\n",
    "# turn off the frame\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_yticks(np.arange(heat.shape[0])+0.5, minor=False)\n",
    "ax.set_xticks(np.arange(heat.shape[1])+0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "# Set the labels\n",
    "\n",
    "# label source:https://en.wikipedia.org/wiki/Basketball_statistics\n",
    "labels_x = ['Topic '+ str(i) for i in range(n_topics)]\n",
    "labels_y = ['Hotel '+ str(i) for i in range(heat.shape[0])]\n",
    "# note I could have used nba_sort.columns but made \"labels\" instead\n",
    "ax.set_xticklabels(labels_x, minor=False) \n",
    "ax.set_yticklabels(labels_y, minor=False)\n",
    "\n",
    "# rotate the \n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "# Turn off all the ticks\n",
    "ax = plt.gca()\n",
    "\n",
    "for t in ax.xaxis.get_major_ticks(): \n",
    "    t.tick1On = False \n",
    "    t.tick2On = False \n",
    "for t in ax.yaxis.get_major_ticks(): \n",
    "    t.tick1On = False \n",
    "    t.tick2On = False "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
